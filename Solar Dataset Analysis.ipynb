{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solar Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ml_utilities as mlutils\n",
    "from loaders import DataLoader\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is the maximum number of unique records that can be obtained with the non-categorical attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 120 feature combinations\n",
      "Maximum number of distinct features and the corresponding feature set:  [(36, [3, 4, 5, 6, 7, 8]), (36, [3, 4, 5, 6, 7, 8, 9])]\n"
     ]
    }
   ],
   "source": [
    "# Generate all possible combinations of 1 < d <=7 features\n",
    "path = 'C:/Users/alexc/OneDrive/Documents/GitHub/Thesis/Data/raw/solar/flare.data.2.txt'\n",
    "features = range(3,10)\n",
    "feature_combinations = []\n",
    "for m in range(2,len(features)+1):\n",
    "    feature_combinations.append(mlutils.findsubsets(features,m))\n",
    "    \n",
    "# How many feature combinations are there?\n",
    "num_combinations = sum([len(x) for x in feature_combinations])\n",
    "print (\"There are\"+\" \"+str(num_combinations)+\" \"+\"feature combinations\")\n",
    "\n",
    "# Load each of the data sets and calculate the number of unique records\n",
    "unique_records_comb = []\n",
    "max_unique_records = 0\n",
    "unique_records_comb_max = []\n",
    "for combination in itertools.chain(*feature_combinations):\n",
    "    # Load data\n",
    "    loader = DataLoader()\n",
    "    loader.load(path,feature_indices=combination,targets=[10])\n",
    "    # Calculate the number of unique records\n",
    "    no_records = mlutils.get_unique_records(loader.features,number=True)\n",
    "    if no_records > max_unique_records:\n",
    "        max_unique_records = no_records\n",
    "    unique_records_comb.append(tuple([no_records,combination]))\n",
    "# Print the feature combinations that generate the maximum number of distinct records\n",
    "for entry in unique_records_comb:\n",
    "        if entry[0] == max_unique_records:\n",
    "            unique_records_comb_max.append(entry)\n",
    "print(\"Maximum number of distinct features and the corresponding feature set: \",unique_records_comb_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the data corresponding to the feature combination that maximises # of unique records and has minimum dimensionality\n",
    "min_len = 11 # init. min length to an aribtrary number > feat. set dimensionality\n",
    "for entry in unique_records_comb_max:\n",
    "    if len(entry[1]) < min_len:\n",
    "        min_len = len(entry[1])\n",
    "for entry in unique_records_comb_max:\n",
    "    if len(entry[1]) == min_len:\n",
    "        features = entry[1]\n",
    "        break\n",
    "loader = DataLoader()\n",
    "loader.load(path,feature_indices=features,target_indices=[10],unique=True)\n",
    "solar_data = loader.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "basepath = 'C:/Users/alexc/OneDrive/Documents/GitHub/Thesis/Data/processed/solar/'\n",
    "with open(basepath+\"solar_data_p.pickle\",\"wb\") as data:\n",
    "    pickle.dump(solar_data,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reload the data set\n",
    "basepath = 'C:/Users/alexc/OneDrive/Documents/GitHub/Thesis/Data/processed/solar/'\n",
    "with open(basepath+\"solar_data_p.pickle\",\"rb\") as data:\n",
    "    solar_data = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 2. 1. 0.]\n",
      " [1. 1. 1. 2. 2. 1. 0.]\n",
      " [1. 1. 2. 2. 2. 1. 0.]\n",
      " [1. 1. 3. 2. 2. 1. 0.]\n",
      " [1. 2. 1. 1. 1. 1. 0.]\n",
      " [1. 2. 1. 1. 2. 1. 0.]\n",
      " [1. 2. 1. 2. 1. 1. 0.]\n",
      " [1. 2. 1. 2. 2. 1. 0.]\n",
      " [1. 2. 1. 2. 2. 2. 1.]\n",
      " [1. 2. 2. 2. 2. 1. 0.]\n",
      " [1. 3. 1. 1. 1. 1. 0.]\n",
      " [1. 3. 1. 1. 2. 1. 0.]\n",
      " [1. 3. 1. 2. 1. 1. 0.]\n",
      " [1. 3. 1. 2. 2. 1. 0.]\n",
      " [1. 3. 1. 2. 2. 2. 2.]\n",
      " [2. 1. 1. 1. 2. 1. 0.]\n",
      " [2. 1. 1. 2. 2. 1. 0.]\n",
      " [2. 1. 3. 2. 2. 1. 5.]\n",
      " [2. 2. 1. 1. 1. 1. 0.]\n",
      " [2. 2. 1. 1. 2. 1. 0.]\n",
      " [2. 2. 1. 2. 2. 1. 0.]\n",
      " [2. 2. 1. 2. 2. 2. 1.]\n",
      " [2. 2. 2. 1. 2. 1. 0.]\n",
      " [2. 2. 2. 2. 2. 1. 0.]\n",
      " [2. 2. 2. 2. 2. 2. 4.]\n",
      " [2. 2. 3. 2. 2. 1. 0.]\n",
      " [2. 2. 3. 2. 2. 2. 0.]\n",
      " [2. 3. 1. 1. 2. 1. 2.]\n",
      " [2. 3. 1. 2. 2. 1. 2.]\n",
      " [2. 3. 1. 2. 2. 2. 0.]\n",
      " [2. 3. 2. 1. 2. 1. 1.]\n",
      " [2. 3. 2. 2. 2. 1. 0.]\n",
      " [2. 3. 3. 1. 2. 1. 0.]\n",
      " [2. 3. 3. 2. 2. 1. 2.]\n",
      " [2. 3. 3. 2. 2. 2. 6.]]\n"
     ]
    }
   ],
   "source": [
    "print (solar_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of database normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.40824829 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.40824829 0.40824829 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.20412415 0.40824829 0.40824829 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.40824829 0.40824829 0.40824829 0.\n",
      "  0.        ]\n",
      " [0.         0.20412415 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.20412415 0.         0.         0.40824829 0.\n",
      "  0.        ]\n",
      " [0.         0.20412415 0.         0.40824829 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.20412415 0.         0.40824829 0.40824829 0.\n",
      "  0.        ]\n",
      " [0.         0.20412415 0.         0.40824829 0.40824829 0.40824829\n",
      "  1.        ]\n",
      " [0.         0.20412415 0.20412415 0.40824829 0.40824829 0.\n",
      "  0.        ]\n",
      " [0.         0.40824829 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.40824829 0.         0.         0.40824829 0.\n",
      "  0.        ]\n",
      " [0.         0.40824829 0.         0.40824829 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.40824829 0.         0.40824829 0.40824829 0.\n",
      "  0.        ]\n",
      " [0.         0.40824829 0.         0.40824829 0.40824829 0.40824829\n",
      "  2.        ]\n",
      " [0.40824829 0.         0.         0.         0.40824829 0.\n",
      "  0.        ]\n",
      " [0.40824829 0.         0.         0.40824829 0.40824829 0.\n",
      "  0.        ]\n",
      " [0.40824829 0.         0.40824829 0.40824829 0.40824829 0.\n",
      "  5.        ]\n",
      " [0.40824829 0.20412415 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.40824829 0.20412415 0.         0.         0.40824829 0.\n",
      "  0.        ]\n",
      " [0.40824829 0.20412415 0.         0.40824829 0.40824829 0.\n",
      "  0.        ]\n",
      " [0.40824829 0.20412415 0.         0.40824829 0.40824829 0.40824829\n",
      "  1.        ]\n",
      " [0.40824829 0.20412415 0.20412415 0.         0.40824829 0.\n",
      "  0.        ]\n",
      " [0.40824829 0.20412415 0.20412415 0.40824829 0.40824829 0.\n",
      "  0.        ]\n",
      " [0.40824829 0.20412415 0.20412415 0.40824829 0.40824829 0.40824829\n",
      "  4.        ]\n",
      " [0.40824829 0.20412415 0.40824829 0.40824829 0.40824829 0.\n",
      "  0.        ]\n",
      " [0.40824829 0.20412415 0.40824829 0.40824829 0.40824829 0.40824829\n",
      "  0.        ]\n",
      " [0.40824829 0.40824829 0.         0.         0.40824829 0.\n",
      "  2.        ]\n",
      " [0.40824829 0.40824829 0.         0.40824829 0.40824829 0.\n",
      "  2.        ]\n",
      " [0.40824829 0.40824829 0.         0.40824829 0.40824829 0.40824829\n",
      "  0.        ]\n",
      " [0.40824829 0.40824829 0.20412415 0.         0.40824829 0.\n",
      "  1.        ]\n",
      " [0.40824829 0.40824829 0.20412415 0.40824829 0.40824829 0.\n",
      "  0.        ]\n",
      " [0.40824829 0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.        ]\n",
      " [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.\n",
      "  2.        ]\n",
      " [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829\n",
      "  6.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Create a data set where the norm of each record has a norm bounded by 1\n",
    "path = 'C:/Users/alexc/OneDrive/Documents/GitHub/Thesis/Data/raw/solar/flare.data.2.txt'\n",
    "features = list(range(3,9))\n",
    "targets = [10]\n",
    "loader = DataLoader()\n",
    "loader.load(path,feature_indices=features,target_indices=targets,unique=True,boundrec=True)\n",
    "solar_data_bound = loader.data\n",
    "print(solar_data_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising the data such that the records has some undersirable effects given this data set. \n",
    "\n",
    "1. For example, in the 5th record, the features 3 and 4 are the same level despite being different before the application (this is all the more obvious if we look at the final row). I am not sure how this would affect the quality of the model predictions \n",
    "\n",
    "2. A lot of the features are also set to zero by this normalisation, which, again raises concerns about whether the data should be transformed in this way\n",
    "\n",
    "3. It is not necessarily obvious how to normalise the targets - limiting to [-1,1] interval is an option?\n",
    "\n",
    "Next steps:\n",
    " - Check empirical covariance matrices for both options and see if they are well-conditioned\n",
    " - Normalise the targets to [-1,1]\n",
    " - Select a testing set (10 samples) and a training set (26 samples) and fit simple regression models\n",
    " to both, see how the results compare and if the transformation makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test whether empirical covariance matrix is invertible (transformed data set)\n",
    "cov_emp = solar_data_bound.T@solar_data_bound\n",
    "precision_emp = np.linalg.inv(cov_emp)\n",
    "# Calculate eigenvalues and eigenvectors of the empirical covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.91965364 -0.21531089 -0.29924656  0.14121472 -0.40219762 -0.03126052\n",
      "  -0.0212258 ]\n",
      " [-0.21531089  0.93265078  0.11541697 -0.11581323 -0.31733734 -0.0903473\n",
      "  -0.00614579]\n",
      " [-0.29924656  0.11541697  1.49402244 -0.22969654 -0.19404047  0.26630593\n",
      "  -0.04997807]\n",
      " [ 0.14121472 -0.11581323 -0.22969654  0.85990531 -0.46392135 -0.31240036\n",
      "  -0.00401983]\n",
      " [-0.40219762 -0.31733734 -0.19404047 -0.46392135  1.00654276 -0.01529334\n",
      "   0.01141577]\n",
      " [-0.03126052 -0.0903473   0.26630593 -0.31240036 -0.01529334  1.49047014\n",
      "  -0.06407051]\n",
      " [-0.0212258  -0.00614579 -0.04997807 -0.00401983  0.01141577 -0.06407051\n",
      "   0.01920298]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
