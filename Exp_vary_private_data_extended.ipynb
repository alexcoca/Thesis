{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of performance of net mechanism on a two-dimensional toy data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import second_moment_experiments_main as experiment\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('text', usetex = True)\n",
    "plt.rc('font', family = 'serif')\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "from exputils import extract_data, initialise_netmech_containers, initialise_adassp_reg_containers, \\\n",
    "initialise_netmech_reg_containers, get_expected_statistics, get_optimal_utilities_statistics\n",
    "from baselines import Regression, DPRegression\n",
    "import numpy as np\n",
    "# %matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "exp_name = 'exp_vary_private_data_extended'\n",
    "with open ('D:/Thesis/Experiments/exp_vary_private_data_extended/' + exp_name, \"rb\") as container:\n",
    "    results = pickle.load(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the performance of the netmechanism for sparse lattices for a wide range of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Setup}$: Draw $25$ samples with the net mechanism set up with $k_f = k_t \\in \\{3:20\\}$ for $100$ different data sets.  Compare the expected average RMSE across the $100$ datasets with the results obtained originally for the seed = 23 case. Then compute the expected average RMSE across the $100$ datasets for the ADASSP algorithm and compare the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default parameters list\n",
    "dimensionality = 2\n",
    "num_records = 300\n",
    "test_frac = 0.8\n",
    "batch_size = 1000\n",
    "directory = 'D:/Thesis/Experiments/exp_vary_private_data_extended/'\n",
    "parallel = True\n",
    "save_data = False\n",
    "partition_method = 'fast_2'\n",
    "workers = 5\n",
    "num_samples = 25\n",
    "sample_parallel = False \n",
    "load_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experiment specific\n",
    "num_points_min = 3\n",
    "min_seed = 8\n",
    "num_points_max = 20\n",
    "num_points_features_vec = range(num_points_min, num_points_max + 1)\n",
    "num_points_targets_vec = range(num_points_min, num_points_max + 1)\n",
    "num_datasets = 100\n",
    "epsilon_vec = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise results data containers\n",
    "results = {key_outer: {key_inner : [] for key_inner in epsilon_vec}  for key_outer in range(num_datasets)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset number 8\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 26.01312045792935\n",
      "Max scaled utility is -0.6151056537961627\n",
      "Generation time 2.0469624996185303\n",
      "Sampling time 0.0\n",
      "Overall max utility -0.41007043586410846\n",
      "Elapsed time with 5 workers is 2.050963878631592\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 50.928407966088\n",
      "Max scaled utility is -0.11826015132702852\n",
      "Generation time 2.1458499431610107\n",
      "Sampling time 0.0\n",
      "Overall max utility -0.07884010088468568\n",
      "Elapsed time with 5 workers is 2.1458499431610107\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 873.2958127405793\n",
      "Max scaled utility is -0.06923984867297145\n",
      "Generation time 2.1698665618896484\n",
      "Sampling time 0.0\n",
      "Overall max utility -0.0461598991153143\n",
      "Elapsed time with 5 workers is 2.1698665618896484\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 2381.472423693861\n",
      "Max scaled utility is -0.07489434620383718\n",
      "Generation time 2.022517204284668\n",
      "Sampling time 0.015595197677612305\n",
      "Overall max utility -0.049929564135891454\n",
      "Elapsed time with 5 workers is 2.0381124019622803\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 10113.508508582732\n",
      "Max scaled utility is -0.04840651533963805\n",
      "Generation time 2.6639797687530518\n",
      "Sampling time 0.008000850677490234\n",
      "Overall max utility -0.03227101022642537\n",
      "Elapsed time with 5 workers is 2.675978422164917\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 18166.30272289063\n",
      "Max scaled utility is -0.026423416633151016\n",
      "Generation time 2.654496192932129\n",
      "Sampling time 0.015622615814208984\n",
      "Overall max utility -0.017615611088767344\n",
      "Elapsed time with 5 workers is 2.670118808746338\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 2\n",
      "Number of batches is  2\n",
      "Starting parallel pool\n",
      "Partition function is 51671.85721039869\n",
      "Max scaled utility is -0.041144346203837286\n",
      "Generation time 2.2642884254455566\n",
      "Sampling time 0.031244516372680664\n",
      "Overall max utility -0.027429564135891524\n",
      "Elapsed time with 5 workers is 2.2955329418182373\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 2\n",
      "Number of batches is  2\n",
      "Starting parallel pool\n",
      "Partition function is 100390.26603735032\n",
      "Max scaled utility is -0.03686654686226009\n",
      "Generation time 2.335186243057251\n",
      "Sampling time 0.05999922752380371\n",
      "Overall max utility -0.024577697908173393\n",
      "Elapsed time with 5 workers is 2.403186321258545\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 4\n",
      "Number of batches is  4\n",
      "Starting parallel pool\n",
      "Partition function is 215506.10231588903\n",
      "Max scaled utility is -0.018689008693295418\n",
      "Generation time 2.6262030601501465\n",
      "Sampling time 0.1715853214263916\n",
      "Overall max utility -0.012459339128863612\n",
      "Elapsed time with 5 workers is 2.797788381576538\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 4\n",
      "Number of batches is  4\n",
      "Starting parallel pool\n",
      "Partition function is 319550.41492145375\n",
      "Max scaled utility is -0.023324098269953006\n",
      "Generation time 3.0884828567504883\n",
      "Sampling time 0.22800016403198242\n",
      "Overall max utility -0.015549398846635337\n",
      "Elapsed time with 5 workers is 3.3204832077026367\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 7\n",
      "Number of batches is  7\n",
      "Starting parallel pool\n",
      "Partition function is 604398.9028705014\n",
      "Max scaled utility is -0.030727679537170698\n",
      "Generation time 2.8742997646331787\n",
      "Sampling time 0.43758201599121094\n",
      "Overall max utility -0.020485119691447132\n",
      "Elapsed time with 5 workers is 3.3118817806243896\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 8\n",
      "Number of batches is  8\n",
      "Starting parallel pool\n",
      "Partition function is 875434.3196406447\n",
      "Max scaled utility is -0.019509730819221895\n",
      "Generation time 3.070885419845581\n",
      "Sampling time 0.5839998722076416\n",
      "Overall max utility -0.013006487212814596\n",
      "Elapsed time with 5 workers is 3.6588850021362305\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 12\n",
      "Number of batches is  12\n",
      "Starting parallel pool\n",
      "Partition function is 1432813.7560820722\n",
      "Max scaled utility is -0.018689008693295418\n",
      "Generation time 3.2719714641571045\n",
      "Sampling time 0.9001760482788086\n",
      "Overall max utility -0.012459339128863612\n",
      "Elapsed time with 5 workers is 4.176146745681763\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 15\n",
      "Number of batches is  15\n",
      "Starting parallel pool\n",
      "Partition function is 2195924.1474861545\n",
      "Max scaled utility is -0.021561012870504014\n",
      "Generation time 3.196000814437866\n",
      "Sampling time 1.207998275756836\n",
      "Overall max utility -0.014374008580336009\n",
      "Elapsed time with 5 workers is 4.407999753952026\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 20\n",
      "Number of batches is  20\n",
      "Starting parallel pool\n",
      "Partition function is 3247796.7261224934\n",
      "Max scaled utility is -0.017449403796162714\n",
      "Generation time 3.49595046043396\n",
      "Sampling time 1.735999345779419\n",
      "Overall max utility -0.011632935864108476\n",
      "Elapsed time with 5 workers is 5.235951900482178\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 24\n",
      "Number of batches is  24\n",
      "Starting parallel pool\n",
      "Partition function is 4447761.356352637\n",
      "Max scaled utility is -0.017643171237500425\n",
      "Generation time 3.99322509765625\n",
      "Sampling time 2.1341583728790283\n",
      "Overall max utility -0.011762114158333616\n",
      "Elapsed time with 5 workers is 6.131862163543701\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 32\n",
      "Number of batches is  32\n",
      "Starting parallel pool\n",
      "Partition function is 6707664.338146526\n",
      "Max scaled utility is -0.018689008693295418\n",
      "Generation time 4.446308851242065\n",
      "Sampling time 2.40800142288208\n",
      "Overall max utility -0.012459339128863612\n",
      "Elapsed time with 5 workers is 6.858311414718628\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 38\n",
      "Number of batches is  38\n",
      "Starting parallel pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition function is 8981418.96487145\n",
      "Max scaled utility is -0.018689008693295418\n",
      "Generation time 5.936959266662598\n",
      "Sampling time 2.7074992656707764\n",
      "Overall max utility -0.012459339128863612\n",
      "Elapsed time with 5 workers is 8.644458532333374\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 11.990707645822928\n",
      "Max scaled utility is -1.2302113075923253\n",
      "Generation time 2.095672130584717\n",
      "Sampling time 0.0\n",
      "Overall max utility -0.41007043586410846\n",
      "Elapsed time with 5 workers is 2.0996809005737305\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 36.69941542912441\n",
      "Max scaled utility is -0.23652030265405705\n",
      "Generation time 2.1011815071105957\n",
      "Sampling time 0.0040013790130615234\n",
      "Overall max utility -0.07884010088468568\n",
      "Elapsed time with 5 workers is 2.1051828861236572\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 522.6013653039929\n",
      "Max scaled utility is -0.1384796973459429\n",
      "Generation time 2.120347261428833\n",
      "Sampling time 0.004001140594482422\n",
      "Overall max utility -0.0461598991153143\n",
      "Elapsed time with 5 workers is 2.1243484020233154\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 1619.2502618088313\n",
      "Max scaled utility is -0.14978869240767437\n",
      "Generation time 2.1239981651306152\n",
      "Sampling time 0.004001140594482422\n",
      "Overall max utility -0.049929564135891454\n",
      "Elapsed time with 5 workers is 2.1319994926452637\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 6312.03717683514\n",
      "Max scaled utility is -0.0968130306792761\n",
      "Generation time 2.130742311477661\n",
      "Sampling time 0.0039997100830078125\n",
      "Overall max utility -0.03227101022642537\n",
      "Elapsed time with 5 workers is 2.138742685317993\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 1\n",
      "Number of batches is  1\n",
      "Starting parallel pool\n",
      "Partition function is 12234.225878996473\n",
      "Max scaled utility is -0.05284683326630203\n",
      "Generation time 2.094785213470459\n",
      "Sampling time 0.008001565933227539\n",
      "Overall max utility -0.017615611088767344\n",
      "Elapsed time with 5 workers is 2.1067869663238525\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 2\n",
      "Number of batches is  2\n",
      "Starting parallel pool\n",
      "Partition function is 32967.98463440196\n",
      "Max scaled utility is -0.08228869240767457\n",
      "Generation time 2.23190975189209\n",
      "Sampling time 0.03200030326843262\n",
      "Overall max utility -0.027429564135891524\n",
      "Elapsed time with 5 workers is 2.2679104804992676\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 2\n",
      "Number of batches is  2\n",
      "Starting parallel pool\n",
      "Partition function is 65466.88492772548\n",
      "Max scaled utility is -0.07373309372452018\n",
      "Generation time 2.3719980716705322\n",
      "Sampling time 0.05999922752380371\n",
      "Overall max utility -0.024577697908173393\n",
      "Elapsed time with 5 workers is 2.431997299194336\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 4\n",
      "Number of batches is  4\n",
      "Starting parallel pool\n",
      "Partition function is 136093.25144877957\n",
      "Max scaled utility is -0.037378017386590835\n",
      "Generation time 2.49113392829895\n",
      "Sampling time 0.1360032558441162\n",
      "Overall max utility -0.012459339128863612\n",
      "Elapsed time with 5 workers is 2.6271371841430664\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 4\n",
      "Number of batches is  4\n",
      "Starting parallel pool\n",
      "Partition function is 209271.13513690105\n",
      "Max scaled utility is -0.04664819653990601\n",
      "Generation time 2.5119965076446533\n",
      "Sampling time 0.2013397216796875\n",
      "Overall max utility -0.015549398846635337\n",
      "Elapsed time with 5 workers is 2.717334032058716\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 7\n",
      "Number of batches is  7\n",
      "Starting parallel pool\n",
      "Partition function is 385842.7718413902\n",
      "Max scaled utility is -0.061455359074341395\n",
      "Generation time 3.0387401580810547\n",
      "Sampling time 0.4519999027252197\n",
      "Overall max utility -0.020485119691447132\n",
      "Elapsed time with 5 workers is 3.4947400093078613\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 8\n",
      "Number of batches is  8\n",
      "Starting parallel pool\n",
      "Partition function is 572303.2159941301\n",
      "Max scaled utility is -0.03901946163844379\n",
      "Generation time 2.9233837127685547\n",
      "Sampling time 0.5999999046325684\n",
      "Overall max utility -0.013006487212814596\n",
      "Elapsed time with 5 workers is 3.523383617401123\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 12\n",
      "Number of batches is  12\n",
      "Starting parallel pool\n",
      "Partition function is 923533.2524912138\n",
      "Max scaled utility is -0.037378017386590835\n",
      "Generation time 3.1480891704559326\n",
      "Sampling time 0.9260797500610352\n",
      "Overall max utility -0.012459339128863612\n",
      "Elapsed time with 5 workers is 4.078169107437134\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 15\n",
      "Number of batches is  15\n",
      "Starting parallel pool\n",
      "Partition function is 1419254.251644091\n",
      "Max scaled utility is -0.04312202574100803\n",
      "Generation time 3.1650257110595703\n",
      "Sampling time 1.291999101638794\n",
      "Overall max utility -0.014374008580336009\n",
      "Elapsed time with 5 workers is 4.4610254764556885\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 20\n",
      "Number of batches is  20\n",
      "Starting parallel pool\n",
      "Partition function is 2091412.8834523608\n",
      "Max scaled utility is -0.03489880759232543\n",
      "Generation time 3.6082041263580322\n",
      "Sampling time 1.654508113861084\n",
      "Overall max utility -0.011632935864108476\n",
      "Elapsed time with 5 workers is 5.262712240219116\n",
      "Initialising synthetic feature space lattice\n",
      "Synthetic feature space initialised\n",
      "Initialising synthethic target space lattice\n",
      "Synthethic target space initialised\n",
      "Number of batches is 24\n",
      "Number of batches is  24\n",
      "Starting parallel pool\n",
      "Partition function is 2891909.019879569\n",
      "Max scaled utility is -0.03528634247500085\n",
      "Generation time 3.581712007522583\n"
     ]
    }
   ],
   "source": [
    "# Collect results\n",
    "for dataset in range(min_seed, num_datasets):\n",
    "    print (\"Dataset number\", dataset)\n",
    "    for epsilon in epsilon_vec:\n",
    "        for num_points_features, num_points_targets in zip(num_points_features_vec, num_points_targets_vec):\n",
    "            results[dataset][epsilon].append(experiment.second_order_moment_experiment(dimensionality = dimensionality, num_records = num_records, \\\n",
    "                                                                                       test_frac = test_frac, batch_size = batch_size,directory = directory, \\\n",
    "                                                                                       parallel = parallel, save_data = save_data,\\\n",
    "                                                                                       partition_method = partition_method, workers = workers, \\\n",
    "                                                                                       num_samples = num_samples, sample_parallel = sample_parallel,\\\n",
    "                                                                                       load_data = load_data, num_points_targets = num_points_targets,\\\n",
    "                                                                                       num_points_features = num_points_features, epsilon = epsilon, \\\n",
    "                                                                                       seed = dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the results for latter access\n",
    "exp_name = 'exp_vary_private_data_extended'\n",
    "with open (directory + exp_name, \"wb\") as container:\n",
    "    pickle.dump(results, container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experimental data containers\n",
    "max_utilities, avg_samples_utility, synthetic_datasets_vec, test_set, private_data = \\\n",
    "    initialise_netmech_containers(epsilon_vec, multiple_datasets = True, max_dataset = num_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract data from the results data structure \n",
    "for dataset_number in results.keys():\n",
    "    # Quantities that are epsilon independent \n",
    "    fixed_eps = 0.1\n",
    "    max_utilities[dataset_number], test_set[dataset_number],private_data[dataset_number] = extract_data(results[dataset_number][fixed_eps],\\\n",
    "                                                                                                    multiple_datasets = True, \\\n",
    "                                                                                                    max_dataset = num_datasets, \\\n",
    "                                                                                                    eps_dependent = False)\n",
    "    # Quantities that are epsilon independent\n",
    "    for epsilon in results[dataset_number].keys():\n",
    "        avg_samples_utility[dataset_number][epsilon], synthetic_datasets_vec[dataset_number][epsilon] = \\\n",
    "           extract_data(results[dataset_number][epsilon], multiple_datasets = True, max_dataset = num_datasets, \\\n",
    "                          eps_dependent = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise containers for the regression on the synthetic data sets released with netmechanism\n",
    "net_mech_reg_coefs, predictive_errs_netmech, min_predictive_errs_netmech, mean_predictive_errs_netmech,\\\n",
    "double_std_predictive_errs_netmech = initialise_netmech_reg_containers(epsilon_vec, multiple_datasets = True,\\\n",
    "                                                                        max_dataset = num_datasets)\n",
    "\n",
    "# Initialise containers for the regression on the synthetic data sets with parameters released by ADASSP\n",
    "adassp_reg_coef, predictive_err_adassp, min_predictive_err_adassp, mean_predictive_err_adassp, double_std_predictive_err_adassp\\\n",
    "                                   = initialise_adassp_reg_containers(epsilon_vec, multiple_datasets = True, max_dataset = num_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit ADASSP to the private dataset and calculate the predictive error\n",
    "for dataset_number in results.keys():\n",
    "    for epsilon in results[dataset_number].keys():\n",
    "        adassp_regressor = DPRegression()\n",
    "        adassp_reg_coef[dataset_number][epsilon] = adassp_regressor.get_parameters(private_data[dataset_number].features, \\\n",
    "                                                                                   private_data[dataset_number].targets,\\\n",
    "                                                                                   num_samples, epsilon, seed = dataset_number)\n",
    "        predictive_err_adassp[dataset_number][epsilon] = Regression().calculate_predictive_error(private_data[dataset_number].test_data, \\\n",
    "                                                                                                 adassp_reg_coef[dataset_number][epsilon])\n",
    "        min_predictive_err_adassp[dataset_number][epsilon] = np.min(predictive_err_adassp[dataset_number][epsilon])\n",
    "        mean_predictive_err_adassp[dataset_number][epsilon] = np.mean(predictive_err_adassp[dataset_number][epsilon])\n",
    "        double_std_predictive_err_adassp[dataset_number][epsilon] = 2*np.std(predictive_err_adassp[dataset_number][epsilon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regression on the datasets released with the net mechanism and calculate predictive error statistics\n",
    "for dataset_number in results.keys():\n",
    "    for epsilon in results[dataset_number].keys():\n",
    "        for synthetic_datasets in synthetic_datasets_vec[dataset_number][epsilon]:\n",
    "            netmech_regressor = Regression()\n",
    "            net_mech_reg_coef = netmech_regressor.fit_data(synthetic_datasets)\n",
    "            net_mech_reg_coefs[dataset_number][epsilon].append(net_mech_reg_coef)\n",
    "            predictive_err_netmech = netmech_regressor.calculate_predictive_error(private_data[dataset_number].test_data, \\\n",
    "                                                                                  net_mech_reg_coef)\n",
    "            predictive_errs_netmech[dataset_number][epsilon].append(predictive_err_netmech)\n",
    "            min_predictive_errs_netmech[dataset_number][epsilon].append(np.min(predictive_err_netmech))\n",
    "            mean_predictive_errs_netmech[dataset_number][epsilon].append(np.mean(predictive_err_netmech))\n",
    "            double_std_predictive_errs_netmech[dataset_number][epsilon].append(2*np.std(predictive_err_netmech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the desired satistics           \n",
    "expected_avg_utility, min_avg_utility, max_avg_utility, expected_mean_predictive_errs_netmech, \\\n",
    "expected_double_std_predictive_errs_netmech, expected_mean_predictive_err_adassp, \\\n",
    "expected_double_std_predictive_err_adassp, expected_min_predictive_errs_netmech, \\\n",
    "min_min_predictive_errs_netmech, max_min_predictive_errs_netmech, expected_min_predictive_err_adassp, \\\n",
    "min_min_predictive_err_adassp, max_min_predictive_err_adassp = get_expected_statistics(avg_samples_utility,\\\n",
    "                                                                    mean_predictive_errs_netmech,\\\n",
    "                                                                    double_std_predictive_errs_netmech,\\\n",
    "                                                                    mean_predictive_err_adassp,\\\n",
    "                                                                    double_std_predictive_err_adassp, \\\n",
    "                                                                    min_predictive_errs_netmech, \\\n",
    "                                                                    min_predictive_err_adassp, \\\n",
    "                                                                    epsilon_vec)\n",
    "expected_optimal_utilities, min_optimal_utilities, max_optimal_utilities = get_optimal_utilities_statistics(max_utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcoordinate = num_points_features_vec\n",
    "# Define the range of epsilon for which various quntities are plotted\n",
    "epsilon_range = [0.1, 0.6, 5.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected average samples utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expected_avg_utility(xcoordinate, expected_avg_samples_utility, expected_optimal_utilities, epsilon):\n",
    "    plt.plot(xcoordinate, expected_avg_samples_utility, 'g--^', label = r'$E[\\mu_u]$' )\n",
    "    plt.plot(xcoordinate, expected_optimal_utilities, 'r*', label = r'$E[opt(u)]$')\n",
    "    plt.xticks(xcoordinate)\n",
    "    plt.xlabel('Lattice denisity (points)', fontsize = 20)\n",
    "    plt.ylabel(r'$\\mu_u$ ($\\varepsilon =' + str(epsilon) + '$)', fontsize = 20)\n",
    "    plt.legend(fontsize = 15)\n",
    "    plt.grid(True)\n",
    "\n",
    "for epsilon in epsilon_range:\n",
    "    plot_expected_avg_utility(xcoordinate, expected_avg_utility[epsilon],\\\n",
    "                            expected_optimal_utilities, epsilon)\n",
    "    plt.plot(xcoordinate, avg_samples_utility[23][epsilon], 'yH', label = r'$\\mu_u^{23}$')\n",
    "    plt.plot(xcoordinate, max_utilities[23], 'cD', label = r'$opt(u)^{23}$')\n",
    "    plt.legend(fontsize = 15)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation of average RMSE with $\\varepsilon$ for various lattice densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xcoordinate)\n",
    "# print(expected_mean_predictive_errs_netmech)\n",
    "print(expected_mean_predictive_err_adassp)\n",
    "print(expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variation of RMSE as a function of epsilon\n",
    "expected_rmse_eps_avg_netmech = {key: [] for key in xcoordinate}\n",
    "expected_rmse_eps_std_netmech = {key: [] for key in xcoordinate}\n",
    "expected_rmse_eps_avg_adassp = []\n",
    "expected_prmse_eps_std_adassp = []\n",
    "\n",
    "# Extract data\n",
    "for epsilon in epsilon_vec[0: len(epsilon_vec)]:\n",
    "    # print (\"DEBUG:\", mean_predictive_errs_netmech[epsilon])\n",
    "    # print (\"DEBUG:\", len(mean_predictive_errs_netmech[epsilon]))\n",
    "    for index in xcoordinate:\n",
    "        expected_rmse_eps_avg_netmech[index].append(expected_mean_predictive_errs_netmech[epsilon][index - xcoordinate[0]])\n",
    "        expected_rmse_eps_std_netmech[index].append(expected_double_std_predictive_errs_netmech[epsilon][index - xcoordinate[0]])\n",
    "\n",
    "expected_rmse_eps_avg_adassp = [expected_mean_predictive_err_adassp[epsilon] for epsilon in epsilon_vec[0: len(epsilon_vec)]] \n",
    "expected_rmse_eps_std_adassp = [expected_double_std_predictive_err_adassp[epsilon] for epsilon in epsilon_vec[0: len(epsilon_vec)]]\n",
    "\n",
    "def plot_expected_rmse_avg_epsilon(epsilon_vec, expected_rmse_eps_avg_netmech, expected_rmse_eps_avg_adassp, lattice_density,\\\n",
    "                                   adassp = True):\n",
    "    plt.semilogx(epsilon_vec, expected_rmse_eps_avg_netmech, '--*', label = 'Net mechansim')\n",
    "    if adassp:\n",
    "        plt.plot(epsilon_vec, expected_rmse_eps_avg_adassp, '--*', label = 'ADASSP')\n",
    "        # plt.ylim([0, 1.5]) \n",
    "    plt.xlabel(r'$\\varepsilon$', fontsize = 18)\n",
    "    # plt.yticks(np.arange(0,1.5, step = 0.1))\n",
    "   #  plt.xticks(epsilon_vec)\n",
    "    plt.ylabel('$E[\\mu_{RMSE}]$ (density = ' + str(lattice_density) +  ' points)', fontsize = 18)\n",
    "    plt.legend(fontsize = 14)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "        \n",
    "for lattice_density in xcoordinate:\n",
    "    # print (\"DEBUG, rmse_eps_avg_netmech\", rmse_eps_avg_netmech[lattice_density])\n",
    "    plot_expected_rmse_avg_epsilon(epsilon_vec[0: len(epsilon_vec)], expected_rmse_eps_avg_netmech[lattice_density],\\\n",
    "                                   expected_rmse_eps_avg_adassp, lattice_density, adassp = True)\n",
    "    \n",
    "# Plot average RMSE for ADASSP for selected values of the lattice density on the same graph\n",
    "plt.figure(1)\n",
    "xcoordinate_range = [3, 4, 5]\n",
    "#xcoordinate_range = xcoordinate\n",
    "for lattice_density in xcoordinate_range:\n",
    "    plt.semilogx(epsilon_vec[0: len(epsilon_vec)], expected_rmse_eps_avg_netmech[lattice_density], '--*', \\\n",
    "                 label = 'density = ' + str(lattice_density))\n",
    "plt.plot(epsilon_vec[0: len(epsilon_vec)], expected_rmse_eps_avg_adassp,'-*', label = 'ADASSP')\n",
    "plt.xlabel(r'$\\varepsilon$', fontsize = 20)\n",
    "plt.ylabel('$E[\\mu_{RMSE}]$', fontsize = 20)\n",
    "plt.xticks(epsilon_vec[0: len(epsilon_vec)])\n",
    "#plt.yticks(np.arange(0,3, step = 0.25))\n",
    "#plt.ylim([0, 3]) \n",
    "plt.grid(True)\n",
    "plt.legend(fontsize = 14, bbox_to_anchor=(0.5, 0.85), bbox_transform = plt.gcf().transFigure, loc = 'center', ncol = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation of the RMSE standard deviation with $\\varepsilon$ for different lattice densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expected_rmse_std_epsilon(epsilon_vec, expected_rmse_eps_std_netmech, expected_rmse_eps_std_adassp, lattice_density,\\\n",
    "                                   adassp = True):\n",
    "    plt.semilogx(epsilon_vec, expected_rmse_eps_std_netmech, '--*', label = 'Net mechansim')\n",
    "    if adassp:\n",
    "        plt.plot(epsilon_vec, expected_rmse_eps_std_adassp, '--*', label = 'ADASSP')\n",
    "        # plt.ylim([0, 1]) \n",
    "    plt.xlabel(r'$\\varepsilon$', fontsize = 18)\n",
    "   # plt.yticks(np.arange(0,5, step = 1))\n",
    "   # plt.ylim([0, 5])\n",
    "   #  plt.xticks(epsilon_vec)\n",
    "    plt.ylabel(r'$\\sigma_{RMSE}$ (density = ' + str(lattice_density) +  ' points)', fontsize = 18)\n",
    "    plt.legend(fontsize = 14)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "        \n",
    "for lattice_density in xcoordinate:\n",
    "    # print (\"DEBUG, expected_rmse_eps_std_netmech\", expected_rmse_eps_std_netmech[lattice_density])\n",
    "    plot_expected_rmse_std_epsilon(epsilon_vec[0: len(epsilon_vec)], expected_rmse_eps_std_netmech[lattice_density],\\\n",
    "                          expected_rmse_eps_std_adassp, lattice_density, adassp = True)\n",
    "\n",
    "# Plot average RMSE for ADASSP for selected values of the lattice density on the same graph\n",
    "plt.figure(1)\n",
    "xcoordinate_range = [3, 4, 5]\n",
    "for lattice_density in xcoordinate_range:\n",
    "    plt.semilogx(epsilon_vec[0: len(epsilon_vec)], expected_rmse_eps_std_netmech[lattice_density], '--*',\\\n",
    "                 label = 'density = ' + str(lattice_density))\n",
    "plt.plot(epsilon_vec[0: len(epsilon_vec)], expected_rmse_eps_std_adassp,'-*', label = 'ADASSP')\n",
    "plt.xlabel(r'$\\varepsilon$', fontsize = 20)\n",
    "plt.ylabel(r'$\\sigma_{RMSE}$', fontsize = 20)\n",
    "plt.xticks(epsilon_vec[0: len(epsilon_vec)])\n",
    "#plt.yticks(np.arange(0, 10, step = 1))\n",
    "#plt.ylim([0, 10]) \n",
    "plt.grid(True)\n",
    "plt.legend(fontsize = 14, bbox_to_anchor=(0.5, 0.85), bbox_transform = plt.gcf().transFigure, loc = 'center', ncol = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expected_optimal_utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent of epsilon, all the experiments return the same utility\n",
    "plt.plot(xcoordinate, expected_optimal_utilities, 'r*', label = r'$E[opt(u)]$')\n",
    "plt.plot(xcoordinate, max_utilities[23], 'cD', label = r'$opt(u)_{23}$')\n",
    "plt.plot(xcoordinate, min_optimal_utilities, 'Xk', label =r'$\\min(opt(u))$' )\n",
    "plt.plot(xcoordinate, max_optimal_utilities, 'Xr', label =r'$\\max(opt(u))$' )\n",
    "plt.xticks(xcoordinate)\n",
    "# plt.yticks(np.arange(-0.07, 0.0, step = 0.01))\n",
    "plt.xlabel('Lattice density (points)', fontsize = 18)\n",
    "plt.ylabel(r'$opt(u)$', fontsize = 18)\n",
    "# plt.ylim([-0.07,0])\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize = 13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected minimum sample error for ADASSP vs netmechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expected_min_predictive_errs_netmech)\n",
    "print(expected_min_predictive_err_adassp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expected_min_rmses(xcoordinate, expected_min_predictive_errs_netmech, expected_min_predictive_err_adassp, epsilon):\n",
    "    plt.plot(xcoordinate, expected_min_predictive_errs_netmech, 'bv', label = 'Net mechanism',  )\n",
    "    plt.plot(xcoordinate, [expected_min_predictive_err_adassp]*len(xcoordinate), 'k^', label = 'ADASSP')\n",
    "    plt.xlabel('Lattice density (points)', fontsize = 18)\n",
    "    plt.ylabel(r'$\\min$ RMSE ($\\varepsilon =' + str(epsilon) + '$)', fontsize = 18)\n",
    "    plt.xticks(xcoordinate)\n",
    "    plt.legend(fontsize = 14)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "for epsilon in epsilon_vec:\n",
    "    plot_expected_min_rmses(xcoordinate, expected_min_predictive_errs_netmech[epsilon], expected_min_predictive_err_adassp[epsilon], \\\n",
    "                   epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum of min sample error for ADASSP vs netmechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data for cross-experiment comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['delta_opt_avg'] = delta_opt_avg\n",
    "data['delta_opt_best'] = delta_opt_best\n",
    "data['max_utilities'] = max_utilities\n",
    "data['rmse_eps_avg_netmech'] = rmse_eps_avg_netmech\n",
    "data['rmse_eps_avg_adassp'] = rmse_eps_avg_adassp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'exp_vary_private_data'\n",
    "with open (directory + filename, 'wb') as container:\n",
    "    pickle.dump(data, container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the raw stats and compare with output of complicated function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper_out = {key: [] for key in epsilon_vec}\n",
    "means = {key: [] for key in epsilon_vec}\n",
    "mins = {key: [] for key in epsilon_vec}\n",
    "maxs = {key: [] for key in epsilon_vec}\n",
    "for dataset_number in avg_samples_utility.keys():\n",
    "    for epsilon in avg_samples_utility[dataset_number].keys():\n",
    "        helper_out[epsilon].append(avg_samples_utility[dataset_number][epsilon])\n",
    "for epsilon in helper_out.keys():\n",
    "    means[epsilon] = np.mean(helper_out[epsilon], axis = 0)\n",
    "    mins[epsilon] = np.min(helper_out[epsilon], axis = 0)\n",
    "    maxs[epsilon] = np.max(helper_out[epsilon], axis = 0)\n",
    "print (means[0.1])\n",
    "print (expected_avg_utility[0.1])\n",
    "for key in helper_out.keys():\n",
    "    assert np.all(expected_avg_utility[key] == means[key])\n",
    "    assert np.all(min_avg_utility[key] == mins[key])\n",
    "    assert np.all(max_avg_utility[key] == maxs[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
