
# -*- coding: utf-8 -*-
"""
Created on Fri Jun 29 18:29:47 2018

@author: alexc
"""

def construct_batches_deprecated(n,k,batch_size):
    '''Function works correctly but two copies of each slice are needed.
    This function does not work correctly. If one sets n=10, k=4 and then,
    creates a list out of e.g. combinations_slices[4], then this actually returns
    the first batch_sizes elements from the iterator (weirdly)'''
    
    
    combinations_slices = []
    
    # Calculate number of batches
    n_batches = math.ceil(comb(n,k,exact=True)/batch_size)
    
    # Construct iterator for combinations
    combinations = itertools.combinations(range(n),k)
    
    # Slice the iterator into n_baches slices
    while len(combinations_slices) < n_batches:
        combinations_slices.append(itertools.islice(combinations,batch_size))
    
    return combinations_slices

def construct_batches(n,k,batch_size):
    ''' Function does not work correctly - see doc for construct_batches_deprecated
    to understand why'''
    
    combinations_slices = []
    
    # Calculate number of batches
    n_batches = math.ceil(comb(n,k,exact=True)/batch_size)
    
    # Construct iterator for combinations
    combinations = itertools.combinations(range(n),k)
    
    # Slice the iterator into n_batches slices. Each slice is duplicated
    # so that it can be subsequently written to a file for later retrieval 
    # during sampling. This is necessary since calling the utility calculation
    # routine exahausts the original slice. If the iterator is first converted to list
    # then storage requirement increases (e.g. from 1KB to 800+ kB for a list of
    # 50,000 tuples of dimension 4)
    
    while len(combinations_slices) < n_batches:
        combinations_slices.append(itertools.tee(itertools.islice(combinations,batch_size)))
        
    return combinations_slices

def evaluate_sample_score(batch_index):
    '''Original code. Investigation revealed that the batches list does not behave correctly,
    retrieving the batch with batch_index is likely not going to return the combinations between
    (batch_index - 1)*batch_size and (batch_index)*batch_size''' 
    
    
    # Storage structure
    struct = {}
    
    # Store combinations to be able to retrieve the correct sample g
    struct['combs']  = batches[batch_index][0]
    
    # Evaluate utility - note that exponential is not taken as sum-exp trick is implemented to 
    # evalute the scores in a numerically stable way during sampling stage
    score_batch = scaling_const*compute_second_moment_utility(features[list(batches[batch_index][1]),:])
    struct ['scores'] = score_batch
    
    # Create data structure which stores the scores for each batch along with 
    # the combinations that generated them
    max_util = np.max(score_batch)
    
    # save the slice object
    filename = "/"+base_filename_s + "_" + str(batch_index)
    save_batch_scores(struct,filename,directory)
    
    # Only max_util is returned in the final version of the code to 
    # allow implementation of exp-normalise trick during sampling . 
    # score_batch is returned for testing purposes
    
    return (max_util,score_batch)


# The test below is no longer necessary as the combinations are not saved any longer
  
# Now let's look at whether the tee reloading works. Well take the example of batch zero

print("Retrieved " + str(len(list(reloaded_data_batches[0]['combs']))) + " combinations for batch 0")

# Test that there is no empty list that we reloaded

combs_lengths = []

for element in reloaded_data:
    combs_lengths.append(len(list(element['combs'])))

# Assert no empty lists
    
assert (0 not in [1,2])

counter = collections.Counter(combs_lengths)

# Calculate outcome space size to ensure correctness
num_targets = targets.shape[0]

outcome_space_size_est = sum([num_targets*x*y for x,y in zip(list(counter.values()),list(counter.keys()))])

outcome_space_size_calc = est_outcome_space_size(features.shape[0],dim,num_points_targ)

assert outcome_space_size_calc == outcome_space_size_est

# OLD Sampling code

def sample_datasets(self, n_batches, num_samples, filenames, partition_function):
        ''' This method samples @num_samples data sets from the space generated by the
        OutcomeSpaceGenerator object. The locations of the files containing unnormalized
        probabilities (scores) of the outcomes are specified in the @filenames list. 
        The partition function is calculated by the OutcomeSpaceGenerator.'''
        
        # Set random number generator for repeatablity 
        np.random.seed(self.seed)
        
        def get_sample(scaled_partition):
            ''' This function subtracts the batch cumulative scores from the 
            (scaled) partition function until the partition function becomes negative. 
            When this occurs, the batch index is stored and a call is made 
            to the get_sample_idxs function to determine the entry in the matrix
            that corresponds to the zero crossing of the scaled partition function.'''
            
            def get_sample_idxs(scores,scaled_partition):
                ''' This function returns the row (row_idx) and column (col_idx)
                index of the entry in the scores matrix for which the partition function
                becomes negative'''
                
                row_idx = 0      
                col_idx = 0 
                
                # Calculate cumulative scores for each batch
                cum_scores = np.sum(scores, axis = 1)
                candidate_partition = scaled_partition
                
                # Step 1: Subtract the batch cumulative scores until scaled partition function
                # becomes negative. Remember the smallest positive value
                while candidate_partition > 0:
                    candidate_partition = scaled_partition - cum_scores[row_idx]
                    if candidate_partition > 0:
                        scaled_partition = candidate_partition 
                        row_idx += 1
                
                # Subtract the entries of the row in which the partition function became
                # negative at Step 1 from the smallest postive value, stopping when the
                # scaled partion becomes negative
                for element in scores[row_idx,:]:
                    scaled_partition -= element
                    if scaled_partition > 0:
                        col_idx += 1
                    else:
                        break
                return (row_idx,col_idx)
            
            # Retrive the saved scores and subtract from the scaled partition function 
            # until it becomes negative. The data corresponding to the score for which
            # this zero crossing occurs is the sampled data set
            
            orig_partition = scaled_partition
            
            for batch in range(n_batches):
                
                # For the 'slow' mode, the sum-exp trick and exponentiation have been applied
                scores = self.retrieve_scores(filenames,batches=[batch])[0]['scaled_utilities']
               
                # Perform sum_exp trick and exponentiate if the fast method has been used for 
                # partition calculations
                if self.partition_method == 'fast':
                    scores = np.exp(scores - self.max_scaled_utility)
                    
                candidate = scaled_partition - np.sum(scores)
                if candidate > 0:
                    scaled_partition = candidate
                else: 
                    row_idx, col_idx = get_sample_idxs(scores,scaled_partition)
                    break
            return (batch, row_idx, col_idx, orig_partition)
       
        def get_batch_id(filename):
            return int(filename[filename.rfind("_") + 1:])
            
        # Store sample indices
        sample_indices = []
         
        # Filenames have to be sorted to ensure correct batch is extracted
        filenames  = sorted(filenames, key = get_batch_id)
        
        for i in range(num_samples):
            
            # To sample, a random number in [0,1] is first generated and multiplied with the partition f
            # (Step 5, in Chapter 4, Section 4.1.3)
            scaled_partition = partition_function*np.random.random()
            # To get a sample, the scores from each batch are subtracted from the scaled partition
            # until the latter becomes negative. The data set for which this zero crossing is 
            # attained is the sampled value ( Step 6, Chapter 4, Section 4.1.3)
            sample_indices.append(get_sample(scaled_partition))
    
        self.sample_indices = sample_indices 

# Old sampling method (08/07)
        
def sample_datasets(self, num_samples, filenames, raw_partition_function, cumulative_partition):
        
        def get_batch_id(filename):
            return int(filename[filename.rfind("_") + 1: ])
        
        # Sort filenames to ensure correct access of stored data
        filenames  = sorted(filenames, key = get_batch_id)
        
        # Scale partition function
        scaled_partitions = raw_partition_function * np.random.random(size=(num_samples,))
        
        # Obtain cumulative partition function - needs to be a numpy array in the actual implementation
        batches = np.searchsorted(cumulative_partition, scaled_partitions)
        
        # Shrink partitions to account for the contribution of all previous batches
        scaled_partitions[batches >= 1] = scaled_partitions[batches >= 1] - cumulative_partition[batches[batches >= 1] - 1]
        
        # This is necessary so that the matrix recovery proceducre can work in general
        self.partition_residuals = scaled_partitions
        
        if not self.sample_parallel:
        
            sample_indices = []
            
            # Create a dictionary with each batch as a separate key, to handle cases when 
            # there are multiple samples from the same batch without loading the data twice
            batch_dictionary = {}
            
            for key,value in zip(batches, scaled_partitions):
                batch_dictionary.setdefault(key, []).append(value)
                
            # For each batch, load the data and calculate the row index
            for key in batch_dictionary.keys():
                if self.load_data:
                    if not self.save_data:
                        raise RuntimeError("Cannot load data that has not be saved. Initialise OutcomeSpaceGenerator with \
                                           save_data = True and then re-run the experiment!")
                    scores = self.retrieve_scores(filenames, batches= [key])[0]['scaled_utilities']
                    if not self.partition_method == 'slow':
                        scores = np.exp(scores)
                else:
                    # Or alternatively regenerate the relevant part of the outcome space on the fly if the data 
                    # has not been stored
                    scores = np.exp(self.generate_batch(key))
                    
                max_row_idx = scores.shape[0] - 1
                max_col_idx = scores.shape[1] - 1
                # Calculate the cumulative scores
#                cum_scores = np.cumsum(np.sum(scores, axis=1))        
#                
#                # Find the rows in the score matrix
#                row_indices = np.searchsorted(cum_scores, batch_dictionary[key])
#                
#                # Rescale partitions to account for the contribution of rows
#                partition_residuals = np.zeros(shape=(len(row_indices,)))
#                partition_residuals[row_indices >= 1] = np.array(batch_dictionary[key])[row_indices >= 1] - \
#                                                        cum_scores[row_indices[row_indices >= 1] - 1]
#                if np.any(row_indices < 1):
#                    partition_residuals[row_indices < 1] = np.array(batch_dictionary[key])[row_indices < 1]
#                    
#                # Determine the column index for each partition residual in the corresponding row
#                col_indices = []
#                for i in range(len(row_indices)):
#                    col_index = np.searchsorted(np.cumsum(scores[row_indices[i],:]), partition_residuals[i])
#                    if  col_index > 0:
#                        col_indices.append(col_index - 1)
#                    else:
#                        col_indices.append(max_col_idx)
#                        row_indices[i] = row_indices[i] - 1
                
                row_indices, col_indices = self.get_sample_coordinates(scores, batch_dictionary[key])
                # Add index tuples to the samples list
                for batch_idx, row_idx, col_idx in zip([key]*len(row_indices), row_indices, col_indices):
                    if int(row_idx) == 0 and int(col_idx) == 0:
                        sample_indices.append((batch_idx - 1, max_row_idx, max_col_idx,0))
                    else:
                        sample_indices.append((batch_idx, int(row_idx), int(col_idx), 0))
        else:
            sample_indices = self.get_samples_parallel(batches)
            # TODO process output of get_samples_parallel
        
        self.sample_indices = sample_indices
    
# 18/07/2018: Sampling algorithm from net mechanism (incorrect...)
        
        
def get_sample_coordinates(self, scores, partition_residuals):
    
    max_col_idx = scores.shape[1] - 1
    # Calculate the cumulative scores
    cum_scores = np.cumsum(np.sum(scores, axis = 1))     
    # Find the rows in the score matrix
    row_indices = np.searchsorted(cum_scores, partition_residuals)
    
    # Rescale partitions to account for the contribution of rows
    rescaled_partitions = np.zeros(shape=(len(row_indices,)))
    rescaled_partitions[row_indices >= 1] = np.array(partition_residuals)[row_indices >= 1] - \
                                            cum_scores[row_indices[row_indices >= 1] - 1]
    if np.any(row_indices < 1):
        rescaled_partitions[row_indices < 1] = np.array(partition_residuals)[row_indices < 1]
        
    # Determine the column index for each partition residual in the corresponding row
    col_indices = []
    for i in range(len(row_indices)):
        col_index = np.searchsorted(np.cumsum(scores[row_indices[i],:]), rescaled_partitions[i])
        if  col_index > 0:
            col_indices.append(col_index - 1)
        else:
            col_indices.append(max_col_idx)
            row_indices[i] = row_indices[i] - 1
            
            
    return (row_indices,col_indices)



