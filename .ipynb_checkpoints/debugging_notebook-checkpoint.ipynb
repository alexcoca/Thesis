{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_generators import ContinuousGenerator\n",
    "import math\n",
    "import testutilities\n",
    "from scipy.special import comb\n",
    "from netmechanism import FeaturesLattice, TargetsLattice\n",
    "import time\n",
    "from itertools import chain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General parameters\n",
    "batch_size = 3000\n",
    "n_private = 300 # 20 for test, 20 to train since test_frac is set to 0.5\n",
    "test_frac = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experiment specific\n",
    "dim = 2 # This won't really work for higher dimensions.\n",
    "# Mesh quality parameters\n",
    "num_points_feat = 5\n",
    "num_points_targ = 5\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of the model from which the private data was generated are [[0.03459577]\n",
      " [0.89392521]]\n"
     ]
    }
   ],
   "source": [
    "# Generate the private data\n",
    "private_data = ContinuousGenerator(d = dim, n = n_private)\n",
    "private_data.generate_data(test_frac = test_frac, seed = 23)\n",
    "print (\"Coefficients of the model from which the private data was generated are\", private_data.coefs)\n",
    "# Calculate its 'contribution' to the utility\n",
    "F_tilde_x = testutilities.get_private_F_tilde(private_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise data\n",
    "#%matplotlib tk\n",
    "#private_data.plot_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the synthetic features and targets\n",
    "OutputLattice = FeaturesLattice()\n",
    "OutputLattice.generate_l2_lattice(dim = dim, num_points = num_points_feat)\n",
    "features = OutputLattice.points\n",
    "OutputLattice2 = TargetsLattice()\n",
    "OutputLattice2.generate_lattice(dim = dim, num_points = num_points_targ)\n",
    "targets = OutputLattice2.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the constant that multiplies the utility to get the score\n",
    "scaled_epsilon = epsilon/2 \n",
    "# Inverse global sensitivity\n",
    "igs = private_data.features.shape[0]/2 \n",
    "# Utility scaling constant \n",
    "scaling_const = igs*scaled_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches is 1\n"
     ]
    }
   ],
   "source": [
    "# Set other parameters necessary for the code to work\n",
    "n_batches = math.ceil(comb(features.shape[0], dim, exact = False)/batch_size)\n",
    "print (\"Number of batches is\", n_batches)\n",
    "experiment_name = 'test_struct_integrity'\n",
    "directory = 'C:/Users/alexc/OneDrive/Documents/GitHub/Thesis/Experiments/' + experiment_name + '/OutcomeSpace'\n",
    "base_filename_s = \"s_eps\" + str(epsilon).replace(\".\", \"\") + \"d\" + str(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for single core processing of this small case is... 0.003999948501586914\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "results = []\n",
    "for batch_index in range(n_batches):\n",
    "    results.append(testutilities.evaluate_sample_score(batch_index, features, targets, scaling_const, F_tilde_x, dim, batch_size, \\\n",
    "                                                       base_filename_s, directory))\n",
    "t_elapsed = time.time()\n",
    "print(\"Time elapsed for single core processing of this small case is...\" + \" \" + str(t_elapsed - t_start))\n",
    "\n",
    "#To Borja:\n",
    "# We process the outcomes in batches. For each batch, a tuple is appended to results. Each tuple contains:\n",
    "# [0]: A scaled version of the maximum utility for that batch. The scaling constant is calculated above in the scaling_const\n",
    "# [1]: A matrix containing the scaled utilities for the batch. For a fixed row index X'X is the same, only X'y changes.\n",
    "# [2]: np.sum(np.exp(scaled_utilities)), a partial sum that we can use to work out the partition function\n",
    "# [3]: A list of tuples with indices. The first index is the batch index, the second and third represent the row and column \n",
    "# corresponding to the max of the matrix of scaled utilities. There are multiple combinations that maximise the scaled utility.\n",
    "# I use this to 'recover' the synthethic data sets and print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recover the synthetic datasets that yields maximum utility\n",
    "synthetic_datasets = testutilities.get_optimal_datasets(results, features, targets, batch_size, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  -0.5 -0.5]\n",
      " [-0.5  0.   0. ]]\n",
      "[[ 0.  -0.5 -0.5]\n",
      " [ 0.5  0.   0. ]]\n",
      "[[ 0.   0.5  0.5]\n",
      " [-0.5  0.   0. ]]\n",
      "[[0.  0.5 0.5]\n",
      " [0.5 0.  0. ]]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# To Borja: print the datasets here. Each dataset is an element in the synthetic_datasets list \n",
    "# Alter range_lim_up, range_lim_low to print specific ones\n",
    "range_lim_low = 0\n",
    "range_lim_up = len(synthetic_datasets)\n",
    "for index in range(range_lim_low, range_lim_up):\n",
    "    print(synthetic_datasets[index])\n",
    "print (len(synthetic_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07376403 0.00409484 0.0062124 ]\n",
      " [0.00409484 0.11673498 0.10449401]]\n"
     ]
    }
   ],
   "source": [
    "# Print F_tilde_x\n",
    "print(F_tilde_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.125 0.    0.   ]\n",
      " [0.    0.125 0.125]]\n",
      "[[0.125 0.    0.   ]\n",
      " [0.    0.125 0.125]]\n",
      "[[0.125 0.    0.   ]\n",
      " [0.    0.125 0.125]]\n",
      "[[0.125 0.    0.   ]\n",
      " [0.    0.125 0.125]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-b6c6dbd8fcf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynthetic_datasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestutilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_synthetic_F_tilde\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtestutilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_synthetic_F_tilde\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynthetic_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msynthetic_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Calculate and print F_tilde_r\n",
    "for dataset in synthetic_datasets:\n",
    "    print(testutilities.get_synthetic_F_tilde(dataset, dim))\n",
    "# print (testutilities.get_synthetic_F_tilde(synthetic_datasets[31], dim))\n",
    "# print (synthetic_datasets[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n",
      "-0.07673498083068855\n"
     ]
    }
   ],
   "source": [
    "for synthetic_dataset in synthetic_datasets:\n",
    "    print(-np.max(np.abs(F_tilde_x - testutilities.get_synthetic_F_tilde(dataset, dim))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the utilities and scores of the recovered datasets. They should be identical for all datasets \n",
    "scores, scaled_utilities, utilities = testutilities.calculate_recovered_scores(synthetic_datasets, F_tilde_x, scaling_const, dim)\n",
    "# Ensure all datasets give the same utility/score/scaled_utility\n",
    "scores = np.array(scores)\n",
    "utilities = np.array(utilities)\n",
    "scaled_utilities = np.array(scaled_utilities)\n",
    "assert np.all(np.isclose(scores - scores[0], 0.0, rtol = 1e-9))\n",
    "assert np.all(np.isclose(utilities - utilities[0], 0.0, rtol = 1e-9))\n",
    "assert np.all(np.isclose(scaled_utilities - scaled_utilities[0], 0.0, rtol = 1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76070084 0.76070084]\n",
      "[-0.02735151 -0.02735151]\n",
      "[-0.27351511 -0.27351511]\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(utilities)\n",
    "print(scaled_utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check the maximum scaled utility matches with the calculated results\n",
    "max_scaled_utilities = []\n",
    "for index in maxima_indices:\n",
    "    max_scaled_utilities.append(results[index][0])\n",
    "assert np.all(np.isclose(max_scaled_utilities - max_scaled_utilities[0], 0.0, rtol = 1e-9))\n",
    "assert np.all(np.isclose(scaled_utilities - max_scaled_utilities[0], 0.0, rtol = 1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
