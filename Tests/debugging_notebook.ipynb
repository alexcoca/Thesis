{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_generators import ContinuousGenerator\n",
    "import math\n",
    "import testutilities\n",
    "from scipy.special import comb\n",
    "from netmechanism import FeaturesLattice, TargetsLattice\n",
    "import time\n",
    "from itertools import chain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General parameters\n",
    "batch_size = 3000\n",
    "n_private = 40 # 20 for test, 20 to train since test_frac is set to 0.5\n",
    "test_frac = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experiment specific\n",
    "dim = 2 # This won't really work for higher dimensions.\n",
    "# Mesh quality parameters\n",
    "num_points_feat = 4\n",
    "num_points_targ = 4\n",
    "epsilon = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of the model from which the private data was generated are [[0.03459577]\n",
      " [0.89392521]]\n"
     ]
    }
   ],
   "source": [
    "# Generate the private data\n",
    "private_data = ContinuousGenerator(d = dim, n = n_private)\n",
    "private_data.generate_data(test_frac = test_frac)\n",
    "print (\"Coefficients of the model from which the private data was generated are\", private_data.coefs)\n",
    "# Calculate its 'contribution' to the utility\n",
    "F_tilde_x = testutilities.get_private_F_tilde(private_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise data\n",
    "%matplotlib tk\n",
    "private_data.plot_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the synthetic features and targets\n",
    "OutputLattice = FeaturesLattice()\n",
    "OutputLattice.generate_l2_lattice(dim = dim, num_points = num_points_feat)\n",
    "features = OutputLattice.points\n",
    "OutputLattice2 = TargetsLattice()\n",
    "OutputLattice2.generate_lattice(dim = dim, num_points = num_points_targ)\n",
    "targets = OutputLattice2.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the constant that multiplies the utility to get the score\n",
    "scaled_epsilon = epsilon/2 \n",
    "# Inverse global sensitivity\n",
    "igs = private_data.features.shape[0]/2 \n",
    "# Utility scaling constant \n",
    "scaling_const = igs*scaled_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches is 1\n"
     ]
    }
   ],
   "source": [
    "# Set other parameters necessary for the code to work\n",
    "n_batches = math.ceil(comb(features.shape[0], dim, exact = False)/batch_size)\n",
    "print (\"Number of batches is\", n_batches)\n",
    "experiment_name = 'test_struct_integrity'\n",
    "directory = 'C:/Users/alexc/OneDrive/Documents/GitHub/Thesis/Experiments/' + experiment_name + '/OutcomeSpace'\n",
    "base_filename_s = \"s_eps\" + str(epsilon).replace(\".\", \"\") + \"d\" + str(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for single core processing of this small case is... 0.0019948482513427734\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "results = []\n",
    "for batch_index in range(n_batches):\n",
    "    results.append(testutilities.evaluate_sample_score(batch_index, features, targets, scaling_const, F_tilde_x, dim, batch_size, \\\n",
    "                                                       base_filename_s, directory))\n",
    "t_elapsed = time.time()\n",
    "print(\"Time elapsed for single core processing of this small case is...\" + \" \" + str(t_elapsed - t_start))\n",
    "\n",
    "#To Borja:\n",
    "# We process the outcomes in batches. For each batch, a tuple is appended to results. Each tuple contains:\n",
    "# [0]: A scaled version of the maximum utility for that batch. The scaling constant is calculated above in the scaling_const\n",
    "# [1]: A matrix containing the scaled utilities for the batch. For a fixed row index X'X is the same, only X'y changes.\n",
    "# [2]: np.sum(np.exp(scaled_utilities)), a partial sum that we can use to work out the partition function\n",
    "# [3]: A list of tuples with indices. The first index is the batch index, the second and third represent the row and column \n",
    "# corresponding to the max of the matrix of scaled utilities. There are multiple combinations that maximise the scaled utility.\n",
    "# I use this to 'recover' the synthethic data sets and print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the synthetic datasets that yields maximum utility\n",
    "synthetic_datasets = testutilities.get_optimal_datasets(results, features, targets, batch_size, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.33333333 -0.33333333 -0.33333333]\n",
      " [-0.33333333  0.33333333  0.33333333]]\n",
      "[[ 0.33333333 -0.33333333 -0.33333333]\n",
      " [ 0.33333333  0.33333333  0.33333333]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# To Borja: print the datasets here. Each dataset is an element in the synthetic_datasets list \n",
    "# Alter range_lim_up, range_lim_low to print specific ones\n",
    "range_lim_low = 0\n",
    "range_lim_up = len(synthetic_datasets)\n",
    "for index in range(range_lim_low, range_lim_up):\n",
    "    print(synthetic_datasets[index])\n",
    "print (len(synthetic_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0837596  0.00640842 0.00862637]\n",
      " [0.00640842 0.100174   0.08976977]]\n"
     ]
    }
   ],
   "source": [
    "# Print F_tilde_x\n",
    "print(F_tilde_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.11111111e-01 0.00000000e+00 2.15876699e-17]\n",
      " [0.00000000e+00 1.11111111e-01 1.11111111e-01]]\n",
      "[[ 1.11111111e-01  0.00000000e+00 -2.15876699e-17]\n",
      " [ 0.00000000e+00  1.11111111e-01  1.11111111e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print F_tilde_r\n",
    "for dataset in synthetic_datasets:\n",
    "    print(testutilities.get_synthetic_F_tilde(dataset, dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the utilities and scores of the recovered datasets. They should be identical for all datasets \n",
    "scores, scaled_utilities, utilities = testutilities.calculate_recovered_scores(synthetic_datasets, F_tilde_x, scaling_const, dim)\n",
    "# Ensure all datasets give the same utility/score/scaled_utility\n",
    "scores = np.array(scores)\n",
    "utilities = np.array(utilities)\n",
    "scaled_utilities = np.array(scaled_utilities)\n",
    "assert np.all(np.isclose(scores - scores[0], 0.0, rtol = 1e-9))\n",
    "assert np.all(np.isclose(utilities - utilities[0], 0.0, rtol = 1e-9))\n",
    "assert np.all(np.isclose(scaled_utilities - scaled_utilities[0], 0.0, rtol = 1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76070084 0.76070084]\n",
      "[-0.02735151 -0.02735151]\n",
      "[-0.27351511 -0.27351511]\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(utilities)\n",
    "print(scaled_utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the maximum scaled utility matches with the calculated results\n",
    "max_scaled_utilities = []\n",
    "for index in maxima_indices:\n",
    "    max_scaled_utilities.append(results[index][0])\n",
    "assert np.all(np.isclose(max_scaled_utilities - max_scaled_utilities[0], 0.0, rtol = 1e-9))\n",
    "assert np.all(np.isclose(scaled_utilities - max_scaled_utilities[0], 0.0, rtol = 1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
